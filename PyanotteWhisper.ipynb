{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8e5795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416e8692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyannote'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwhisper\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mdatetime\u001B[39;00m\n\u001B[1;32m      6\u001B[0m start \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyannote\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token=True)\u001B[39;00m\n\u001B[1;32m     11\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyannote/speaker-diarization\u001B[39m\u001B[38;5;124m\"\u001B[39m, use_auth_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhf_elGAMVFpKQGpBfHCDlRzlrAKXlXybnPaeY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pyannote'"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "# available_pipelines = [p.modelId for p in HfApi().list_models(filter=\"pyannote-audio-pipeline\")]\n",
    "# list(filter(lambda p: p.startswith(\"pyannote/\"), available_pipelines))\n",
    "\n",
    "import whisper, json, datetime\n",
    "start = datetime.datetime.now()\n",
    "from pyannote.audio import Pipeline\n",
    "  \n",
    "# pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True)\n",
    "# pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', use_auth_token=True)\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"\")\n",
    "\n",
    "# Check si CUDA ok\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(f'{datetime.datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cf1a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['anger_26-09-2016.wav_result']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pathfile=\"./AlainWOrk/140322/audio2.wav\"\n",
    "pathfile=\"./Anger/anger_26-09-2016.wav\"\n",
    "\n",
    "# Creation des dossiers pour l'enregistrement des resultats \n",
    "import pathlib, os, json\n",
    "path = pathlib.Path(pathfile)\n",
    "dir_result = f\"{path.parent.absolute()}/{path.name}_result\"\n",
    "\n",
    "pathlib.Path(f\"{dir_result}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.listdir(path.parent.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0edb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarization_result ok in 0:13:01.557050\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "diarization_result = pipeline(pathfile)\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.diarization.json\", 'w') as d:\n",
    "    json.dump(diarization_result.for_json(), d, ensure_ascii=False, indent=4)\n",
    "d.close()\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.diarization.txt\", 'w') as t:\n",
    "    t.write(f\"{diarization_result}\")\n",
    "t.close()\n",
    "\n",
    "print(f'diarization_result ok in {datetime.datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6a99e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "model = whisper.load_model(\"large-v2\")\n",
    "print(f\"Model chargé en {datetime.datetime.now() - start} Fait chauffer le gpu Marcel ! \")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "whisper_res = model.transcribe(pathfile, verbose=True, language=\"French\")\n",
    "print(f\"whisper_res OK in {datetime.datetime.now() - start}\")\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.whisper.json\", 'w') as f:\n",
    "    json.dump(whisper_res, f, ensure_ascii=False, indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850cf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "PUNC_SENT_END = []\n",
    "# PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)\n",
    "        \n",
    "        \n",
    "def add_space_before_punctuation(text):\n",
    "    text = re.sub(r'(\\w)([?!])', r'\\1 \\2', text)\n",
    "    return text\n",
    "\n",
    "def remove_space_and_tiret(text):\n",
    "    text = re.sub(r'^\\ ', '',text)\n",
    "    text = re.sub(r'^\\— ', '',text)\n",
    "    return text\n",
    "\n",
    "def replace_words(text):\n",
    "\n",
    "    word_dict = {\n",
    "        \"que l'on doit\": \"que nous devons\",\n",
    "        \"on propose\": \"nous proposons\",\n",
    "        \"on restait\": \"nous restions\",\n",
    "        \"on restera\": \"nous resterons\",\n",
    "        \"on reste\": \"nous restons\",\n",
    "        \"on reprend\": \"nous reprenons\",\n",
    "        \"qu'on s'était\": \"que nous nous étions\",\n",
    "        \"qu'on s'est\": \"que nous nous sommes\",\n",
    "        \"on se rend\": \"nous nous rendons\",\n",
    "        \"on vous suit\": \"nous vous suivons\",\n",
    "        \"on pourrait\": \"nous pourrions\",\n",
    "        \"on peut\": \"nous pouvons\",\n",
    "        \"on ne saura\": \"nous ne saurons\",\n",
    "        \"on peut s'en\": \"nous pouvons nous en \",\n",
    "        \"il va falloir\": \"il faudra\",\n",
    "        \"on n'avait\": \"nous n'avions \",\n",
    "        \"qu'on ferait\": \"que nous ferions\",\n",
    "        \"c'est des\": \"ce sont des\",\n",
    "        \"c'était pas\": \"ce n'était pas\",\n",
    "        \"j'ai pas\": \"je n'ai pas\",\n",
    "        \"il a pas \": \"il n'a pas\",\n",
    "        \"c'est des\": \"ce sont des\",\n",
    "        \"qu'on n'est plus\": \"que nous ne sommes plus\",\n",
    "        \"qu'on n'est\": \"que nous sommes\",\n",
    "        \"qu'on est\": \"que nous sommes\",\n",
    "        \"qu'on a\": \"que nous avons\",\n",
    "        \"qu'on n'a plus\": \"que nous n'avons plus\",\n",
    "        \"on en avait\": \"nous en avions\",\n",
    "        \"on en a\": \"nous en avons\",\n",
    "        \"on tenait\": \"nous tenions\",\n",
    "        \"il faut pas\": \"il ne faut pas\",\n",
    "        \"il doit pas\": \"il ne doit pas\",\n",
    "        \"il travaille pas\": \"il ne travaille pas\",\n",
    "        \"on n'est pas\": \"nous ne sommes pas\",\n",
    "        \"qu'on puisse\": \"que nous puissions\",\n",
    "        \"est-ce que vous avez\": \"avez-vous\",\n",
    "        \"est-ce que vous êtes \": \"etes-vous\",\n",
    "        \"est-ce qu'il serait\": \"serait-il\",\n",
    "        \"on ne s'est pas\": \"nous ne nous sommes pas\",\n",
    "        \"qu'on n'était pas\": \"que nous n'étions pas\",\n",
    "        \"qu'on était pas\": \"que nous n'étions pas\",\n",
    "        \"qu'on ne mettait pas\": \"que nous ne mettions pas\",\n",
    "        \"qu'on mettait pas\": \"que nous ne mettions pas\",\n",
    "        \"on peut se\": \"nous pouvons nous\",\n",
    "        \"qu'on en a\": \"que nous en avons\",\n",
    "        \"est-ce qu'on peut\": \"pouvons-nous\",\n",
    "        \"on a\": \"nous avons\",\n",
    "        \"on est\": \"nous sommes\",\n",
    "        \"on vient\": \"nous venons\",\n",
    "        \"on va\": \"nous allons\",\n",
    "        \"on obtient\": \"nous obtenons\",\n",
    "        \"on fait\": \"nous faisons\",\n",
    "        \"on coordonne\": \"nous coordonnons\",\n",
    "        \"on se frotte\": \"nous nous frottons\",\n",
    "        \"on se donne\": \"nous nous donnons\",\n",
    "        \"on mange\": \"nous mangeons\",\n",
    "        \"on donne\": \"nous donnons\",\n",
    "        \"on apprend\": \"nous apprenons\",\n",
    "        \"on s'est dit\": \"nous nous sommes dit\",\n",
    "        \"on sait\": \"nous savons\",\n",
    "        \"on prend\": \"nous prenons\",\n",
    "        \"ça\": \"cela\",\n",
    "        \"est-ce que vous avez\": \"avez-vous\",\n",
    "        \"est-ce qu'il y a\": \"y a-t-il \",\n",
    "        \"puisqu'on voit\": \"puisque nous voyons\",\n",
    "        \"qu'on voit\": \"que nous voyons\",\n",
    "        \"comment on peut\": \"comment pouvons-nous\",\n",
    "        \"on participe\": \"nous participons\",\n",
    "        \"-là\": \"\",\n",
    "        \"qu'nous\": \"que nous\",\n",
    "        \"tu ne m'as pas\": \"vous ne m'avez pas\",\n",
    "        \"tu as\": \"vous avez\",\n",
    "        \"tu fais\": \"vous faites\",\n",
    "        \"tu es\": \"vous êtes\",\n",
    "        \"tu dis\": \"vous dites\",\n",
    "        \"tu parles\": \"vous parlez\",\n",
    "        \"tu rentres\": \"vous rentrez\",\n",
    "        \"tu avais\": \"vous aviez\",\n",
    "        \"tu n'avais pas\": \"vous n'aviez pas\",\n",
    "    }\n",
    "    \n",
    "\n",
    "    for old_word, new_word in word_dict.items():\n",
    "        pattern = r'\\b(' + old_word + r')\\b'                   \n",
    "        pattern_case_insensitive = re.compile(pattern, re.IGNORECASE)\n",
    "        text = remove_space_and_tiret(text)\n",
    "        text = add_space_before_punctuation(text)\n",
    "        text = re.sub(pattern_case_insensitive, lambda m: new_word.capitalize() if m.group(1)[0].isupper() else new_word.lower(), text)\n",
    "    return text                                                             \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da742e2b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pathfile=\"./jmarc/jmarc.wav\"\n",
    "# path = pathlib.Path(pathfile)\n",
    "# dir_result = f\"{path.parent.absolute()}/{path.name}_result\"\n",
    "\n",
    "# with open(f\"{dir_result}/{path.name}.whisper.json\", 'r') as f:\n",
    "#     whisper_res_from_json = json.load(f)\n",
    "\n",
    "# with open(f\"{dir_result}/{path.name}.diarization.json\", 'r') as d:\n",
    "#     diarization_result_fron_json = json.load(d)\n",
    "    \n",
    "# print(type(diarization_result))\n",
    "# print(type(diarization_result_fron_json))\n",
    "# print(type(whisper_res))\n",
    "# print(type(whisper_res_from_json))\n",
    "    \n",
    "final_result = diarize_text(whisper_res, diarization_result)\n",
    "with open(f\"{dir_result}/{path.name}.final.txt\", 'w') as r:\n",
    "    ex_speaker = None\n",
    "    for segment, speaker, text in final_result:\n",
    "        if speaker != ex_speaker:\n",
    "            r.write('\\n\\n')\n",
    "            r.write(f'{speaker}')\n",
    "            r.write('\\n\\n')\n",
    "            ex_speaker = speaker\n",
    "        r.write(replace_words(text))\n",
    "r.close()\n",
    "\n",
    "# final = open(f\"{dir_result}/{path.name}.final.txt\", 'rt')\n",
    "# print(final.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "581bf0ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speakers_list = []\n",
    "\n",
    "list_txt = []\n",
    "ex_speaker = None\n",
    "text = {'whisper_text' : '', 'replace_text' : ''}\n",
    "for segment, speaker, original_text in final_result:\n",
    "    if speaker != ex_speaker:\n",
    "        list_txt.append(text)\n",
    "        text = {\n",
    "            'speaker': f'{speaker}', \n",
    "            'segment' : f'{segment}', \n",
    "            'whisper_text' : '', \n",
    "            'replace_text': '',\n",
    "        }\n",
    "        ex_speaker = speaker\n",
    "        speakers_list.append(speaker)\n",
    "    text['whisper_text'] += original_text\n",
    "    text['replace_text'] += replace_words(original_text)\n",
    "\n",
    "\n",
    "    \n",
    "result_dict = {f'{pathfile}' : {\n",
    "                   \"speakers\": [(speaker, '') for speaker in list(set(speakers_list))],\n",
    "                   \"results\" : list_txt\n",
    "                    }\n",
    "              }\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.final_dict.json\", 'w') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False, indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25bea88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def gpt3edit(instruction=None, prompt=None):\n",
    "    response = openai.Edit.create(\n",
    "      model=\"text-davinci-edit-001\",\n",
    "      input=f\"{prompt}\",\n",
    "      instruction=f\"{instruction}\",\n",
    "      temperature=0,\n",
    "      top_p=1\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def gpt35turbo(system=None, prompt=None):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{system}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "edit = \"corrige les fautes d'orthographe, de grammaire et de syntaxe.\"\n",
    "\n",
    "Correction = \"Tu dois corriger les fautes d'orthographe et de syntaxe comme le fait une stenographe \"\\\n",
    "\"tout en gardant le texte similaire à la réalité. \"\\\n",
    "\"Aucun résumé, aucun avis, tu dois rester EXTREMEMENT fidèle au texte d'origine. \" \\\n",
    "\"Le sens ne doit jamais être altéré. \"\n",
    "\n",
    "Resume = \"Tu es chargé du compte rendu de réunions.\"\\\n",
    "         \"Résume chaque intervention\"\\\n",
    "         \" par une liste des arguments, idées ou pensée de cette façon : \"\\\n",
    "        \" - idée 1\"\\\n",
    "        \" - idée 2\"\\\n",
    "        \" - idée etc ....\"\\\n",
    "\n",
    "classification = \"Classification de texte : Veuillez classer le texte fourni dans une catégorie spécifique en fonction de son contenu. Les catégories peuvent inclure des sujets tels que les actualités, la politique, la technologie, la santé, l'éducation, etc.\"\n",
    "\n",
    "entite = \"Veuillez analyser le texte fourni et identifier toutes les entités nommées telles que les personnes, les organisations, les lieux, les dates, etc.\"\n",
    "\n",
    "sentiment = \"Veuillez analyser le texte fourni et déterminer l'attitude générale exprimée dans le texte. Le texte peut être positif, négatif ou neutre.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fea5049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1917 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "27 / 247\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "The model: `text-davinci-edit-001` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidRequestError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 22\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlist_txt\u001B[38;5;241m.\u001B[39mindex(txt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(list_txt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;66;03m# txt['STENO'] = gpt35turbo(system=Correction, prompt=f\"{txt['whisper_text']}\")\u001B[39;00m\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;66;03m# txt['RESUME'] = gpt35turbo(system=Resume, prompt=f\"{txt['whisper_text']}\")\u001B[39;00m\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;66;03m# txt['CLASSIFICATION']= gpt35turbo(system=classification, prompt=f\"{txt['STENO']}\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;66;03m# txt['ENTITE']= gpt35turbo(system=entite, prompt=f\"{txt['STENO']}\")\u001B[39;00m\n\u001B[1;32m     21\u001B[0m         \u001B[38;5;66;03m# txt['SENTIMENT']= gpt35turbo(system=sentiment, prompt=f\"{txt['STENO']}\")\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m         txt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEDIT\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m=\u001B[39m \u001B[43mgpt3edit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstruction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mtxt\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwhisper_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#         print(\"\")\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#         print(\"ORIGINAL\")\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m#         print(txt.get('speaker'))\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m#         print(txt['RESUME'])\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m#         print(\"\")\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[12], line 7\u001B[0m, in \u001B[0;36mgpt3edit\u001B[0;34m(instruction, prompt)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgpt3edit\u001B[39m(instruction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m----> 7\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEdit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-davinci-edit-001\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mprompt\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m      \u001B[49m\u001B[43minstruction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43minstruction\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/testouille-y1TiEclG-py3.10/lib/python3.10/site-packages/openai/api_resources/edit.py:28\u001B[0m, in \u001B[0;36mEdit.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/testouille-y1TiEclG-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/testouille-y1TiEclG-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:226\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    207\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    214\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    215\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    216\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    217\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    218\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    224\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    225\u001B[0m     )\n\u001B[0;32m--> 226\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/testouille-y1TiEclG-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:619\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    612\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    613\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    614\u001B[0m         )\n\u001B[1;32m    615\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[1;32m    616\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    618\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 619\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    625\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    626\u001B[0m     )\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/testouille-y1TiEclG-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:682\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    680\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    683\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    684\u001B[0m     )\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mInvalidRequestError\u001B[0m: The model: `text-davinci-edit-001` does not exist"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# tokenizer(\"Hello world\")['input_ids']\n",
    "\n",
    "with open(f\"/home/slaan/Gits/SummerWhisper/anger_26-09-2016.wav_result/anger_26-09-2016.wav.final_dict.json\", 'r', encoding='utf8') as f:\n",
    "    result_dict_loaded = json.load(f)\n",
    "f.close()\n",
    "list_txt = result_dict_loaded[f\"{pathfile}\"]['results']\n",
    "print(len(list_txt))\n",
    "\n",
    "\n",
    "for txt in list_txt[26:29]:\n",
    "    txt['num'] = list_txt.index(txt)\n",
    "    txt['token'] = len(tokenizer(txt['whisper_text'])['input_ids'])\n",
    "    if txt['token'] > 60 :\n",
    "        print(f'{list_txt.index(txt)} / {len(list_txt)}')\n",
    "        # txt['STENO'] = gpt35turbo(system=Correction, prompt=f\"{txt['whisper_text']}\")\n",
    "        # txt['RESUME'] = gpt35turbo(system=Resume, prompt=f\"{txt['whisper_text']}\")\n",
    "        # txt['CLASSIFICATION']= gpt35turbo(system=classification, prompt=f\"{txt['STENO']}\")\n",
    "        # txt['ENTITE']= gpt35turbo(system=entite, prompt=f\"{txt['STENO']}\")\n",
    "        # txt['SENTIMENT']= gpt35turbo(system=sentiment, prompt=f\"{txt['STENO']}\")\n",
    "        txt['EDIT']= gpt3edit(instruction=edit, prompt=f\"{txt['whisper_text']}\")\n",
    "#         print(\"\")\n",
    "#         print(\"ORIGINAL\")\n",
    "#         print(txt.get('speaker'))\n",
    "#         print(txt.get('whisper_text'))\n",
    "#         print(\"OCCURENCE\")\n",
    "#         print(txt.get('replace_text'))\n",
    "#         print(\"TURBO STENO\")\n",
    "#         print(txt['STENO'] )\n",
    "#         print(\"TURBO RESUME\")\n",
    "#         print(txt['RESUME'])\n",
    "#         print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d6ada",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: https://github.com/promptslab/Promptify\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11e44a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in list_txt:\n",
    "    txt['num'] = list_txt.index(txt)\n",
    "    \n",
    "result_dict = {f'{pathfile}' : {\n",
    "                   \"speakers\": [(speaker, '') for speaker in list(set(speakers_list))],\n",
    "                   \"results\" : list_txt\n",
    "                    }\n",
    "              }\n",
    "\n",
    "with open(f\"{dir_result}/{path.name}.final_dict.json\", 'w', encoding=(\"utf8\")) as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
